{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape'; // For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\n\nexport function conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n  let out;\n  const intermediates = []; // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n\n  const batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) && sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD; // The algorithm in the if condition assumes (1) the output will be packed,\n  // (2) x is packed, (3) x isChannelsLast, (4)  x's packed texture is already\n  // on GPU, (5) col is odd, (6) the width, height and inChannels are the same\n  // for xTexData.shape and xShape.\n\n  const canOptimize = !batchMatMulWillBeUnpacked && xTexData.isPacked && isChannelsLast && xTexData.texture != null && xShape[2] % 2 !== 0 && util.arraysEqual(xTexData.shape.slice(-3), xShape.slice(-3));\n\n  if (canOptimize) {\n    // We avoid expensive packed 2x2 reshape by padding col count to next,\n    // even number. When col is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for next even col. We make the odd-cols tensor to look like\n    // even-cols tensor before the operation and, after the batchMatMul,\n    // fix the even-cols result to have odd number of cols.\n    const targetShape = xShape[0] * xShape[1] * (xShape[2] + 1);\n    const xReshaped = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    }; // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing col count, after batchMatMul->...->compileProgram leads to\n    // invalid col count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even col count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), () => `packed reshape ${xTexData.shape} to ${xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(pointwiseConvTexData.isPacked, () => 'batchMatMul result is expected to be packed'); // Restore the input shape to original.\n\n    xTexData.shape = originalXTexDataShape; // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n\n    pointwiseConvTexData.shape = convInfo.outShape;\n    out = identity({\n      inputs: {\n        x: pointwiseConv\n      },\n      backend\n    });\n    out.shape = convInfo.outShape;\n    intermediates.push(pointwiseConv);\n  } else {\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] : xShape[0] * xShape[2] * xShape[3];\n    const xReshaped = reshape({\n      inputs: {\n        x\n      },\n      backend,\n      attrs: {\n        shape: [1, targetShape, convInfo.inChannels]\n      }\n    });\n    const filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    const result = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      transposeA,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n    out = reshape({\n      inputs: {\n        x: result\n      },\n      backend,\n      attrs: {\n        shape: convInfo.outShape\n      }\n    });\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  }\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n} // Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\n\nexport function conv2dWithIm2Row({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n  const isChannelsLast = dataFormat === 'channelsLast';\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n  const intermediates = [];\n  const xSqueezed = reshape({\n    inputs: {\n      x\n    },\n    backend,\n    attrs: {\n      shape: x.shape.slice(1)\n    }\n  });\n  const w2Row = reshape({\n    inputs: {\n      x: filter\n    },\n    backend,\n    attrs: {\n      shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]\n    }\n  });\n  intermediates.push(xSqueezed);\n  intermediates.push(w2Row);\n  const im2ColProgram = new Im2ColPackedProgram(x2ColShape, convInfo);\n  const customValues = [xSqueezed.shape, [convInfo.padInfo.top, convInfo.padInfo.left], [convInfo.strideHeight, convInfo.strideWidth], [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inChannels], [convInfo.filterWidth * convInfo.inChannels], [convInfo.outWidth]];\n  const im2Col = backend.runWebGLProgram(im2ColProgram, [xSqueezed], 'float32', customValues);\n  const im2ColReshaped = reshape({\n    inputs: {\n      x: im2Col\n    },\n    backend,\n    attrs: {\n      shape: [1, x2ColShape[0], x2ColShape[1]]\n    }\n  });\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(im2ColReshaped.shape, w2Row.shape, [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs = [im2ColReshaped, w2Row];\n\n  if (bias) {\n    inputs.push(bias);\n  }\n\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  const outShape = isChannelsLast ? [1, outHeight, outWidth, convInfo.outChannels] : [1, convInfo.outChannels, outHeight, outWidth];\n  const out = reshape({\n    inputs: {\n      x: product\n    },\n    backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(product);\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}","map":{"version":3,"sources":["../../src/kernels/Conv2D_impl.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAkC,IAAlC,QAA6C,uBAA7C;AAGA,SAAQ,mBAAR,QAAkC,sBAAlC;AACA,SAAQ,4BAAR,QAA2C,oCAA3C;AACA,SAAQ,mBAAR,QAAkC,sBAAlC;AACA,OAAO,KAAK,UAAZ,MAA4B,eAA5B;AAEA,SAAQ,eAAR,EAAyB,2BAAzB,QAA2D,oBAA3D;AACA,SAAQ,QAAR,QAAuB,YAAvB;AACA,SAAQ,OAAR,QAAsB,WAAtB,C,CAaA;AACA;AACA;;AACA,OAAM,SAAU,cAAV,CAAyB;AAC7B,EAAA,CAD6B;AAE7B,EAAA,MAF6B;AAG7B,EAAA,QAH6B;AAI7B,EAAA,OAJ6B;AAK7B,EAAA,IAAI,GAAG,IALsB;AAM7B,EAAA,sBAAsB,GAAG,IANI;AAO7B,EAAA,cAAc,GAAG,CAPY;AAQ7B,EAAA,UAAU,GAAG;AARgB,CAAzB,EASS;AACb;AACA;AACA,QAAM,MAAM,GAAG,CAAC,CAAC,KAAjB;AACA,QAAM,QAAQ,GAAG,OAAO,CAAC,OAAR,CAAgB,GAAhB,CAAoB,CAAC,CAAC,MAAtB,CAAjB;AACA,QAAM,eAAe,GAAG,QAAQ,CAAC,UAAjC;AACA,QAAM,WAAW,GAAG,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,GAAwB,MAAM,CAAC,CAAD,CAAlD;AACA,QAAM,gBAAgB,GAAG,QAAQ,CAAC,WAAlC;AACA,QAAM,cAAc,GAAG,QAAQ,CAAC,UAAT,KAAwB,cAA/C;AACA,QAAM,UAAU,GAAG,KAAnB;AACA,QAAM,UAAU,GAAG,KAAnB;AAEA,MAAI,GAAJ;AACA,QAAM,aAAa,GAAiB,EAApC,CAba,CAeb;AACA;;AACA,QAAM,yBAAyB,GAC3B,CAAC,WAAW,KAAK,CAAhB,IAAqB,gBAAgB,KAAK,CAA3C,KACA,eAAe,GAAG,2BAFtB,CAjBa,CAqBb;AACA;AACA;AACA;;AACA,QAAM,WAAW,GAAG,CAAC,yBAAD,IAA8B,QAAQ,CAAC,QAAvC,IAChB,cADgB,IACE,QAAQ,CAAC,OAAT,IAAoB,IADtB,IAC8B,MAAM,CAAC,CAAD,CAAN,GAAY,CAAZ,KAAkB,CADhD,IAEhB,IAAI,CAAC,WAAL,CAAiB,QAAQ,CAAC,KAAT,CAAe,KAAf,CAAqB,CAAC,CAAtB,CAAjB,EAA2C,MAAM,CAAC,KAAP,CAAa,CAAC,CAAd,CAA3C,CAFJ;;AAIA,MAAI,WAAJ,EAAiB;AACf;AACA;AACA;AACA;AACA;AACA;AACA,UAAM,WAAW,GAAG,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,IAAyB,MAAM,CAAC,CAAD,CAAN,GAAY,CAArC,CAApB;AACA,UAAM,SAAS,GAAe;AAC5B,MAAA,MAAM,EAAE,CAAC,CAAC,MADkB;AAE5B,MAAA,KAAK,EAAE,CAAC,CAAD,EAAI,WAAJ,EAAiB,QAAQ,CAAC,UAA1B,CAFqB;AAG5B,MAAA,KAAK,EAAE,CAAC,CAAC;AAHmB,KAA9B,CARe,CAaf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AACA,UAAM,qBAAqB,GAAG,QAAQ,CAAC,KAAvC;AACA,IAAA,QAAQ,CAAC,KAAT,GAAiB,QAAQ,CAAC,KAAT,CAAe,KAAf,EAAjB;AACA,IAAA,QAAQ,CAAC,KAAT,CAAe,QAAQ,CAAC,KAAT,CAAe,MAAf,GAAwB,CAAvC;AACA,IAAA,IAAI,CAAC,MAAL,CACI,UAAU,CAAC,aAAX,CAAyB,QAAQ,CAAC,KAAlC,EAAyC,SAAS,CAAC,KAAnD,CADJ,EAEI,MAAM,kBAAkB,QAAQ,CAAC,KAAK,OAClC,SAAS,CAAC,KAAK,aAHvB;AAIA,UAAM,cAAc,GAAG,OAAO,CAAC;AAC7B,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE;AAAJ,OADqB;AAE7B,MAAA,OAF6B;AAG7B,MAAA,KAAK,EAAE;AAAC,QAAA,KAAK,EAAE,CAAC,CAAD,EAAI,QAAQ,CAAC,UAAb,EAAyB,QAAQ,CAAC,WAAlC;AAAR;AAHsB,KAAD,CAA9B;AAKA,IAAA,aAAa,CAAC,IAAd,CAAmB,cAAnB;AACA,UAAM,aAAa,GAAG,eAAe,CAAC;AACpC,MAAA,CAAC,EAAE,SADiC;AAEpC,MAAA,CAAC,EAAE,cAFiC;AAGpC,MAAA,OAHoC;AAIpC,MAAA,UAJoC;AAKpC,MAAA,UALoC;AAMpC,MAAA,IANoC;AAOpC,MAAA,UAPoC;AAQpC,MAAA,sBARoC;AASpC,MAAA;AAToC,KAAD,CAArC;AAYA,UAAM,oBAAoB,GAAG,OAAO,CAAC,OAAR,CAAgB,GAAhB,CAAoB,aAAa,CAAC,MAAlC,CAA7B;AACA,IAAA,IAAI,CAAC,MAAL,CACI,oBAAoB,CAAC,QADzB,EAEI,MAAM,6CAFV,EA/Ce,CAkDf;;AACA,IAAA,QAAQ,CAAC,KAAT,GAAiB,qBAAjB,CAnDe,CAoDf;AACA;;AACA,IAAA,oBAAoB,CAAC,KAArB,GAA6B,QAAQ,CAAC,QAAtC;AAEA,IAAA,GAAG,GAAG,QAAQ,CAAC;AAAC,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE;AAAJ,OAAT;AAA6B,MAAA;AAA7B,KAAD,CAAd;AACA,IAAA,GAAG,CAAC,KAAJ,GAAY,QAAQ,CAAC,QAArB;AAEA,IAAA,aAAa,CAAC,IAAd,CAAmB,aAAnB;AACD,GA5DD,MA4DO;AACL,UAAM,WAAW,GAAG,cAAc,GAAG,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,GAAwB,MAAM,CAAC,CAAD,CAAjC,GACG,MAAM,CAAC,CAAD,CAAN,GAAY,MAAM,CAAC,CAAD,CAAlB,GAAwB,MAAM,CAAC,CAAD,CADnE;AAEA,UAAM,SAAS,GAAG,OAAO,CAAC;AACxB,MAAA,MAAM,EAAE;AAAC,QAAA;AAAD,OADgB;AAExB,MAAA,OAFwB;AAGxB,MAAA,KAAK,EAAE;AAAC,QAAA,KAAK,EAAE,CAAC,CAAD,EAAI,WAAJ,EAAiB,QAAQ,CAAC,UAA1B;AAAR;AAHiB,KAAD,CAAzB;AAKA,UAAM,cAAc,GAAG,OAAO,CAAC;AAC7B,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE;AAAJ,OADqB;AAE7B,MAAA,OAF6B;AAG7B,MAAA,KAAK,EAAE;AAAC,QAAA,KAAK,EAAE,CAAC,CAAD,EAAI,QAAQ,CAAC,UAAb,EAAyB,QAAQ,CAAC,WAAlC;AAAR;AAHsB,KAAD,CAA9B;AAKA,UAAM,MAAM,GAAG,eAAe,CAAC;AAC7B,MAAA,CAAC,EAAE,SAD0B;AAE7B,MAAA,CAAC,EAAE,cAF0B;AAG7B,MAAA,UAH6B;AAI7B,MAAA,UAJ6B;AAK7B,MAAA,OAL6B;AAM7B,MAAA,IAN6B;AAO7B,MAAA,UAP6B;AAQ7B,MAAA,sBAR6B;AAS7B,MAAA;AAT6B,KAAD,CAA9B;AAYA,IAAA,GAAG,GAAG,OAAO,CACT;AAAC,MAAA,MAAM,EAAE;AAAC,QAAA,CAAC,EAAE;AAAJ,OAAT;AAAsB,MAAA,OAAtB;AAA+B,MAAA,KAAK,EAAE;AAAC,QAAA,KAAK,EAAE,QAAQ,CAAC;AAAjB;AAAtC,KADS,CAAb;AAGA,IAAA,aAAa,CAAC,IAAd,CAAmB,SAAnB;AACA,IAAA,aAAa,CAAC,IAAd,CAAmB,cAAnB;AACA,IAAA,aAAa,CAAC,IAAd,CAAmB,MAAnB;AACD;;AAED,OAAK,MAAM,CAAX,IAAgB,aAAhB,EAA+B;AAC7B,IAAA,OAAO,CAAC,6BAAR,CAAsC,CAAtC;AACD;;AAED,SAAO,GAAP;AACD,C,CAED;AACA;;AACA,OAAM,SAAU,gBAAV,CAA2B;AAC/B,EAAA,CAD+B;AAE/B,EAAA,MAF+B;AAG/B,EAAA,QAH+B;AAI/B,EAAA,OAJ+B;AAK/B,EAAA,IAAI,GAAG,IALwB;AAM/B,EAAA,sBAAsB,GAAG,IANM;AAO/B,EAAA,cAAc,GAAG,CAPc;AAQ/B,EAAA,UAAU,GAAG;AARkB,CAA3B,EASS;AACb;AACA;AACA;AACA;AACA;AACA;AACA,QAAM;AACJ,IAAA,WADI;AAEJ,IAAA,YAFI;AAGJ,IAAA,UAHI;AAIJ,IAAA,QAJI;AAKJ,IAAA,SALI;AAMJ,IAAA;AANI,MAOF,QAPJ;AASA,QAAM,cAAc,GAAG,UAAU,KAAK,cAAtC;AAEA,QAAM,SAAS,GAAG,WAAW,GAAG,YAAd,GAA6B,UAA/C;AACA,QAAM,OAAO,GAAG,SAAS,GAAG,QAA5B;AACA,QAAM,UAAU,GAAG,CAAC,SAAD,EAAY,OAAZ,CAAnB;AACA,QAAM,UAAU,GAAG,IAAnB;AACA,QAAM,UAAU,GAAG,KAAnB;AAEA,QAAM,aAAa,GAAiB,EAApC;AAEA,QAAM,SAAS,GACX,OAAO,CAAC;AAAC,IAAA,MAAM,EAAE;AAAC,MAAA;AAAD,KAAT;AAAc,IAAA,OAAd;AAAuB,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE,CAAC,CAAC,KAAF,CAAQ,KAAR,CAAc,CAAd;AAAR;AAA9B,GAAD,CADX;AAEA,QAAM,KAAK,GAAG,OAAO,CAAC;AACpB,IAAA,MAAM,EAAE;AAAC,MAAA,CAAC,EAAE;AAAJ,KADY;AAEpB,IAAA,OAFoB;AAGpB,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE,CAAC,CAAD,EAAI,SAAJ,EAAe,IAAI,CAAC,aAAL,CAAmB,MAAM,CAAC,KAA1B,IAAmC,SAAlD;AAAR;AAHa,GAAD,CAArB;AAMA,EAAA,aAAa,CAAC,IAAd,CAAmB,SAAnB;AACA,EAAA,aAAa,CAAC,IAAd,CAAmB,KAAnB;AAEA,QAAM,aAAa,GAAG,IAAI,mBAAJ,CAAwB,UAAxB,EAAoC,QAApC,CAAtB;AACA,QAAM,YAAY,GAAG,CACnB,SAAS,CAAC,KADS,EACF,CAAC,QAAQ,CAAC,OAAT,CAAiB,GAAlB,EAAuB,QAAQ,CAAC,OAAT,CAAiB,IAAxC,CADE,EAEnB,CAAC,QAAQ,CAAC,YAAV,EAAwB,QAAQ,CAAC,WAAjC,CAFmB,EAGnB,CAAC,QAAQ,CAAC,cAAV,EAA0B,QAAQ,CAAC,aAAnC,CAHmB,EAGgC,CAAC,QAAQ,CAAC,UAAV,CAHhC,EAInB,CAAC,QAAQ,CAAC,WAAT,GAAuB,QAAQ,CAAC,UAAjC,CAJmB,EAI2B,CAAC,QAAQ,CAAC,QAAV,CAJ3B,CAArB;AAMA,QAAM,MAAM,GAAG,OAAO,CAAC,eAAR,CACX,aADW,EACI,CAAC,SAAD,CADJ,EACiB,SADjB,EAC4B,YAD5B,CAAf;AAEA,QAAM,cAAc,GAAG,OAAO,CAAC;AAC7B,IAAA,MAAM,EAAE;AAAC,MAAA,CAAC,EAAE;AAAJ,KADqB;AAE7B,IAAA,OAF6B;AAG7B,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE,CAAC,CAAD,EAAI,UAAU,CAAC,CAAD,CAAd,EAAmB,UAAU,CAAC,CAAD,CAA7B;AAAR;AAHsB,GAAD,CAA9B;AAMA,EAAA,aAAa,CAAC,IAAd,CAAmB,MAAnB;AACA,EAAA,aAAa,CAAC,IAAd,CAAmB,cAAnB;AAEA,QAAM,OAAO,GAAG,IAAI,IAAI,IAAxB;AACA,QAAM,yBAAyB,GAAG,sBAAsB,IAAI,IAA5D;AACA,QAAM,iBAAiB,GAAG,UAAU,KAAK,WAAzC;AACA,QAAM,eAAe,GACjB,UAAU,GAAG,4BAA4B,CAAC,UAAD,EAAa,IAAb,CAA/B,GAAoD,IADlE;AAEA,QAAM,aAAa,GAAG,IAAI,mBAAJ,CAClB,cAAc,CAAC,KADG,EAElB,KAAK,CAAC,KAFY,EAGlB,CAAC,CAAD,EAAI,OAAJ,EAAa,QAAQ,CAAC,WAAtB,CAHkB,EAGkB,UAHlB,EAG8B,UAH9B,EAG0C,OAH1C,EAIlB,eAJkB,EAID,yBAJC,EAI0B,iBAJ1B,CAAtB;AAKA,QAAM,MAAM,GAAiB,CAAC,cAAD,EAAiB,KAAjB,CAA7B;;AACA,MAAI,IAAJ,EAAU;AACR,IAAA,MAAM,CAAC,IAAP,CAAY,IAAZ;AACD;;AACD,MAAI,yBAAJ,EAA+B;AAC7B,IAAA,MAAM,CAAC,IAAP,CAAY,sBAAZ;AACD;;AACD,MAAI,iBAAJ,EAAuB;AACrB,UAAM,eAAe,GAAG,OAAO,CAAC,cAAR,CACpB,EADoB,EAChB,SADgB,EAEpB,IAAI,CAAC,iBAAL,CAAuB,cAAvB,EAA0D,SAA1D,CAFoB,CAAxB;AAGA,IAAA,MAAM,CAAC,IAAP,CAAY,eAAZ;AACA,IAAA,aAAa,CAAC,IAAd,CAAmB,eAAnB;AACD;;AACD,QAAM,OAAO,GAAG,OAAO,CAAC,eAAR,CAAwB,aAAxB,EAAuC,MAAvC,EAA+C,SAA/C,CAAhB;AAEA,QAAM,QAAQ,GAAG,cAAc,GAC3B,CAAC,CAAD,EAAI,SAAJ,EAAe,QAAf,EAAyB,QAAQ,CAAC,WAAlC,CAD2B,GAE3B,CAAC,CAAD,EAAI,QAAQ,CAAC,WAAb,EAA0B,SAA1B,EAAqC,QAArC,CAFJ;AAGA,QAAM,GAAG,GACL,OAAO,CAAC;AAAC,IAAA,MAAM,EAAE;AAAC,MAAA,CAAC,EAAE;AAAJ,KAAT;AAAuB,IAAA,OAAvB;AAAgC,IAAA,KAAK,EAAE;AAAC,MAAA,KAAK,EAAE;AAAR;AAAvC,GAAD,CADX;AAGA,EAAA,aAAa,CAAC,IAAd,CAAmB,OAAnB;;AACA,OAAK,MAAM,CAAX,IAAgB,aAAhB,EAA+B;AAC7B,IAAA,OAAO,CAAC,6BAAR,CAAsC,CAAtC;AACD;;AAED,SAAO,GAAP;AACD","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendWebGL} from '../backend_webgl';\nimport {Im2ColPackedProgram} from '../im2col_packed_gpu';\nimport {mapActivationToShaderProgram} from '../kernel_utils/kernel_funcs_utils';\nimport {MatMulPackedProgram} from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\n\nimport {batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD} from './BatchMatMul_impl';\nimport {identity} from './Identity';\nimport {reshape} from './Reshape';\n\ntype Conv2DConfig = {\n  x: TensorInfo,\n  filter: TensorInfo,\n  convInfo: backend_util.Conv2DInfo,\n  backend: MathBackendWebGL,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n\n  let out: TensorInfo;\n  const intermediates: TensorInfo[] = [];\n\n  // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n  const batchMatMulWillBeUnpacked =\n      (outerShapeX === 1 || outerShapeFilter === 1) &&\n      sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n\n  // The algorithm in the if condition assumes (1) the output will be packed,\n  // (2) x is packed, (3) x isChannelsLast, (4)  x's packed texture is already\n  // on GPU, (5) col is odd, (6) the width, height and inChannels are the same\n  // for xTexData.shape and xShape.\n  const canOptimize = !batchMatMulWillBeUnpacked && xTexData.isPacked &&\n      isChannelsLast && xTexData.texture != null && xShape[2] % 2 !== 0 &&\n      util.arraysEqual(xTexData.shape.slice(-3), xShape.slice(-3));\n\n  if (canOptimize) {\n    // We avoid expensive packed 2x2 reshape by padding col count to next,\n    // even number. When col is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for next even col. We make the odd-cols tensor to look like\n    // even-cols tensor before the operation and, after the batchMatMul,\n    // fix the even-cols result to have odd number of cols.\n    const targetShape = xShape[0] * xShape[1] * (xShape[2] + 1);\n    const xReshaped: TensorInfo = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    };\n    // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing col count, after batchMatMul->...->compileProgram leads to\n    // invalid col count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even col count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(\n        webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape),\n        () => `packed reshape ${xTexData.shape} to ${\n            xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(\n        pointwiseConvTexData.isPacked,\n        () => 'batchMatMul result is expected to be packed');\n    // Restore the input shape to original.\n    xTexData.shape = originalXTexDataShape;\n    // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n    pointwiseConvTexData.shape = convInfo.outShape;\n\n    out = identity({inputs: {x: pointwiseConv}, backend});\n    out.shape = convInfo.outShape;\n\n    intermediates.push(pointwiseConv);\n  } else {\n    const targetShape = isChannelsLast ? xShape[0] * xShape[1] * xShape[2] :\n                                         xShape[0] * xShape[2] * xShape[3];\n    const xReshaped = reshape({\n      inputs: {x},\n      backend,\n      attrs: {shape: [1, targetShape, convInfo.inChannels]}\n    });\n    const filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n    const result = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      transposeA,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n\n    out = reshape(\n        {inputs: {x: result}, backend, attrs: {shape: convInfo.outShape}});\n\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  }\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}\n\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n\n  const isChannelsLast = dataFormat === 'channelsLast';\n\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n\n  const intermediates: TensorInfo[] = [];\n\n  const xSqueezed =\n      reshape({inputs: {x}, backend, attrs: {shape: x.shape.slice(1)}});\n  const w2Row = reshape({\n    inputs: {x: filter},\n    backend,\n    attrs: {shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]}\n  });\n\n  intermediates.push(xSqueezed);\n  intermediates.push(w2Row);\n\n  const im2ColProgram = new Im2ColPackedProgram(x2ColShape, convInfo);\n  const customValues = [\n    xSqueezed.shape, [convInfo.padInfo.top, convInfo.padInfo.left],\n    [convInfo.strideHeight, convInfo.strideWidth],\n    [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inChannels],\n    [convInfo.filterWidth * convInfo.inChannels], [convInfo.outWidth]\n  ];\n  const im2Col = backend.runWebGLProgram(\n      im2ColProgram, [xSqueezed], 'float32', customValues);\n  const im2ColReshaped = reshape({\n    inputs: {x: im2Col},\n    backend,\n    attrs: {shape: [1, x2ColShape[0], x2ColShape[1]]}\n  });\n\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation =\n      activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(\n      im2ColReshaped.shape as [number, number, number],\n      w2Row.shape as [number, number, number],\n      [1, numCols, convInfo.outChannels], transposeA, transposeB, hasBias,\n      fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs: TensorInfo[] = [im2ColReshaped, w2Row];\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo(\n        [], 'float32',\n        util.createScalarValue(leakyreluAlpha as {} as 'float32', 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n\n  const outShape = isChannelsLast ?\n      [1, outHeight, outWidth, convInfo.outChannels] :\n      [1, convInfo.outChannels, outHeight, outWidth];\n  const out =\n      reshape({inputs: {x: product}, backend, attrs: {shape: outShape}});\n\n  intermediates.push(product);\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}