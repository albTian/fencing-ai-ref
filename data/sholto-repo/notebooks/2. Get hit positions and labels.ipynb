{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387bce6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 05:09:52,570\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor, ceil\n",
    "import json\n",
    "import ray\n",
    "import time\n",
    "N_CPUS = 1\n",
    "ray.init(num_cpus = N_CPUS)\n",
    "\n",
    "FPS = 25\n",
    "CLIP_LEN = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6616daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     \"abT8l2NV9Bk\": {\n",
    "#         \"annotations\": {\n",
    "#             \"label\": \"pulling espresso shot\",\n",
    "#             \"segment\": [\n",
    "#                 126.0,\n",
    "#                 136.0\n",
    "#             ]\n",
    "#         },\n",
    "#         \"duration\": 10.0,\n",
    "#         \"subset\": \"validate\",\n",
    "#         \"url\": \"https://www.youtube.com/watch?v=abT8l2NV9Bk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36522d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "OT_L = [337,348, 234,250]\n",
    "OT_R =  [337,348, 390,406]\n",
    "score_R = [330,334, 380,500]\n",
    "score_L = [330,334, 140,260]\n",
    "\n",
    "num_left = [310,325, 265,285]\n",
    "num_right = [310,325, 355,375]\n",
    "\n",
    "green_box = cv.imread(\"assets/greenbox.png\")\n",
    "red_box = cv.imread(\"assets/redbox.png\")\n",
    "white_box = cv.imread(\"assets/whitebox.png\")\n",
    "\n",
    "\n",
    "def left(frame):\n",
    "    if (np.sum(abs(frame[score_L[0]:score_L[1], score_L[2]:score_L[3]].astype(int)-red_box.astype(int))) <= 40000):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def right(frame):\n",
    "    if (np.sum(abs(frame[score_R[0]:score_R[1], score_R[2]:score_R[3]].astype(int)-green_box.astype(int))) <= 40000):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def check_for_score(frame):\n",
    "    if left(frame) and right(frame):\n",
    "        return 'both'\n",
    "    elif left(frame) or right(frame):\n",
    "        return 'one'\n",
    "    else:\n",
    "        return 'neither'\n",
    "    \n",
    "def go_to_start_of_light(cap, frame):\n",
    "    '''\n",
    "    Because we are skipping 10 frames at a time, we need to jump back to when the light first came on\n",
    "    '''\n",
    "    while check_for_score(frame) != 'neither':\n",
    "        current_frame = cap.get(cv.CAP_PROP_POS_FRAMES)\n",
    "        cap.set(cv.CAP_PROP_POS_FRAMES,current_frame-2)\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "def jump_to_blockout_time(cap):\n",
    "    current_frame = cap.get(cv.CAP_PROP_POS_FRAMES)\n",
    "    cap.set(cv.CAP_PROP_POS_FRAMES,current_frame+20)\n",
    "    ret, frame = cap.read()\n",
    "    return frame\n",
    "\n",
    "def jump_past_hit(cap, frame):\n",
    "    while check_for_score(frame) != 'neither':\n",
    "        current_frame = cap.get(cv.CAP_PROP_POS_FRAMES)\n",
    "        cap.set(cv.CAP_PROP_POS_FRAMES,current_frame+2)\n",
    "        ret, frame = cap.read()\n",
    "    return frame\n",
    "\n",
    "class clip:\n",
    "#     vid_id: str,\n",
    "#     label: str,\n",
    "#     start: float,\n",
    "#     end: float,\n",
    "#     start_frame: int,\n",
    "#     end_frame: int,\n",
    "        \n",
    "    def __init__(self, vid_id, end_frame, final_img = None, label=None):\n",
    "        self.vid_id = vid_id\n",
    "        self.end_frame = end_frame\n",
    "        if final_img is not None:\n",
    "            self.final_img = final_img\n",
    "        self.label = label\n",
    "        \n",
    "\n",
    "def store_clip(video_id, last_frame_idx, frame, clips):\n",
    "    if len(clips) > 0:\n",
    "        if clips[-1].label == None:\n",
    "            print('No label, discarding')\n",
    "            clips.pop() # we never managed to get a label for it\n",
    "    clips.append(clip(video_id, last_frame_idx, frame))\n",
    "    \n",
    "def save_json(clips, vid_id):\n",
    "    json_out = {}\n",
    "\n",
    "    for i,c in enumerate(clips):\n",
    "        if c.label != None:\n",
    "            start, end = c.end_frame-CLIP_LEN, c.end_frame\n",
    "            start_time, end_time = start/FPS, end/FPS\n",
    "            start_floor, end_ceil = float(floor(start_time)), float(ceil(end_time))\n",
    "            c.vid_id = c.vid_id.replace(\".mp4\", \"\")\n",
    "            json_out[f\"{c.vid_id}_{i}\"] = {\n",
    "                    \"annotations\": {\"label\":f\"{c.label}\", \"segment\": [start_time, end_time]},\n",
    "                    \"annotations_frame\":  {\"label\":f\"{c.label}\", \"segment\": [start, end]},\n",
    "                    \"annotations_rounded\": {\"label\":f\"{c.label}\", \"segment\": [start_floor, end_ceil]},\n",
    "                    \"duration\": end_time-start_time,\n",
    "                    \"duration_frame\":end-start,\n",
    "                    \"duration_rounded\": end_ceil-start_floor,\n",
    "                    \"subset\": \"train\",\n",
    "                    \"url\": f'https://www.youtube.com/watch?v={c.vid_id}'\n",
    "            }\n",
    "            \n",
    "    with open(f'../per_vid_labels/{vid_id.replace(\".mp4\", \"\")}.json', 'w') as outfile:\n",
    "        json.dump(json_out, outfile)\n",
    "\n",
    "\n",
    "# @ray.remote\n",
    "class labeller(object):\n",
    "    '''\n",
    "    Create a labeller 'actor' so that we can use ray by creating 1 per CPU, then feed it video labels\n",
    "    Necessary due to the keras model for the scoreboard\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        from tensorflow import keras\n",
    "        self.model = keras.models.load_model('../score_classifier')\n",
    "        \n",
    "    def score(self,img):\n",
    "        grey = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        thr = cv.threshold(grey, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)[1]\n",
    "        img = thr[np.newaxis, :,:, np.newaxis]\n",
    "        return int(np.argmax(self.model(img)))\n",
    "    \n",
    "    \n",
    "    def label(self,old_frame, current_frame):\n",
    "        left_current = self.score(current_frame[num_left[0]:num_left[1], num_left[2]:num_left[3]])\n",
    "        right_current  = self.score(current_frame[num_right[0]:num_right[1], num_right[2]:num_right[3]])\n",
    "        left_old = self.score(old_frame[num_left[0]:num_left[1], num_left[2]:num_left[3]])\n",
    "        right_old  = self.score(old_frame[num_right[0]:num_right[1], num_right[2]:num_right[3]])\n",
    "\n",
    "        if left_current-left_old == 1 and right_current-right_old == 0:\n",
    "            return 'left'\n",
    "        elif right_current-right_old == 1 and left_current-left_old == 0:\n",
    "            return 'right'\n",
    "        elif right_current-right_old == 0 and left_current-left_old == 0:\n",
    "            return 'together'\n",
    "        else:\n",
    "            return None # unclear what it is, pop the clip later\n",
    "        \n",
    "    def maybe_label_last(self, clips, frame, vid_id):\n",
    "        if len(clips) > 0:\n",
    "            if clips[-1].label == None:\n",
    "                clips[-1].label = self.label(clips[-1].final_img, frame)\n",
    "                print(clips[-1].label, vid_id)\n",
    "            \n",
    "    def not_0_or_15(self, frame):\n",
    "        '''\n",
    "        Avoids catching the weapons test at the beginning or random shit in longer videos\n",
    "        '''\n",
    "        if self.score(frame[num_left[0]:num_left[1], num_left[2]:num_left[3]]) == 0 and self.score(frame[num_right[0]:num_right[1], num_right[2]:num_right[3]]) == 0:\n",
    "            return False\n",
    "        elif self.score(frame[num_left[0]:num_left[1], num_left[2]:num_left[3]]) == 15 or self.score(frame[num_right[0]:num_right[1], num_right[2]:num_right[3]]) == 15:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # This is going to be a little tricky\n",
    "    # Basically, whenever we detect a light, record the score, then the blockout time to see whether the other person also hit is 25ms, so we have to jump\n",
    "    # ahead 7 frames, but lets make it 10 to be safe. \n",
    "    # if that is both, then record the time up till then and hold onto this clip\n",
    "    # at the next light check to see if the score has increased, if so, label the prev\n",
    "    def label_video(self, vid_id):\n",
    "        v= f'../precut/{vid_id}'\n",
    "        print(v)\n",
    "        if os.path.exists(f'../per_vid_labels/{vid_id.replace(\".mp4\", \".json\")}'):\n",
    "            print(\"Already have labels\")\n",
    "        else:\n",
    "            cap = cv.VideoCapture(v)\n",
    "            cap.set(cv.CAP_PROP_POS_FRAMES,1000)\n",
    "            clips = []\n",
    "\n",
    "            while cap.isOpened():\n",
    "                current_frame = cap.get(cv.CAP_PROP_POS_FRAMES)\n",
    "                cap.set(cv.CAP_PROP_POS_FRAMES,current_frame+10) # jump 10 frames at a time\n",
    "                ret, frame = cap.read()\n",
    "                # if frame is read correctly ret is True\n",
    "                if not ret:\n",
    "                    print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "                    break\n",
    "\n",
    "#                 cv.imshow('frame', frame)\n",
    "                if check_for_score(frame) != 'neither':\n",
    "\n",
    "                    # label the last clip now we've got a new light\n",
    "                    self.maybe_label_last(clips, frame, vid_id)\n",
    "\n",
    "                    current_frame = cap.get(cv.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "                    # jump ahead by the blockout time when we see a light, to ensure we see both lights if they are on\n",
    "                    frame = jump_to_blockout_time(cap)\n",
    "\n",
    "                    # if both lights are on, store the clip\n",
    "                    if check_for_score(frame) == 'both' and self.not_0_or_15(frame): # then its a hit which required disambiguation\n",
    "                        # as we may be a few frames in, go to the beginnging, then jump forward to the blockout time precisely\n",
    "                        go_to_start_of_light(cap, frame)\n",
    "                        # jump ahead by the blockout time when we see a light, to ensure we get both\n",
    "                        frame = jump_to_blockout_time(cap)\n",
    "                        store_clip(vid_id, cap.get(cv.CAP_PROP_POS_FRAMES), frame, clips)\n",
    "                        # skip the part where both are on\n",
    "                        frame = jump_past_hit(cap, frame)\n",
    "                    elif check_for_score(frame) == 'one':\n",
    "                        pass #\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                if cv.waitKey(1) == ord('q'):\n",
    "                    break\n",
    "\n",
    "            try:\n",
    "                save_json(clips, vid_id)\n",
    "            except:\n",
    "                print(f\"Error with {vid_id}\")\n",
    "            cap.release()\n",
    "        os.remove(v) # Once we've processed it, remove it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee65c8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precut/rEqkpD4upOE.mp4\n",
      "left rEqkpD4upOE.mp4\n",
      "right rEqkpD4upOE.mp4\n",
      "left rEqkpD4upOE.mp4\n",
      "right rEqkpD4upOE.mp4\n",
      "right rEqkpD4upOE.mp4\n",
      "together rEqkpD4upOE.mp4\n",
      "left rEqkpD4upOE.mp4\n",
      "right rEqkpD4upOE.mp4\n",
      "right rEqkpD4upOE.mp4\n",
      "together rEqkpD4upOE.mp4\n",
      "right rEqkpD4upOE.mp4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-49d0ae049650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabeller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-ae3d0850b15d>\u001b[0m in \u001b[0;36mlabel_video\u001b[1;34m(self, vid_id)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                     \u001b[1;31m# if both lights are on, store the clip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mcheck_for_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'both'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_0_or_15\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# then its a hit which required disambiguation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                         \u001b[1;31m# as we may be a few frames in, go to the beginnging, then jump forward to the blockout time precisely\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                         \u001b[0mgo_to_start_of_light\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ae3d0850b15d>\u001b[0m in \u001b[0;36mcheck_for_score\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcheck_for_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'both'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ae3d0850b15d>\u001b[0m in \u001b[0;36mleft\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscore_L\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mscore_L\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_L\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mscore_L\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mred_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m40000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "vids = os.listdir(\"../precut\")\n",
    "import random\n",
    "random.shuffle(vids)\n",
    "a = labeller()\n",
    "a.label_video(vids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f922b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util import ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949eaca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "actors =  ActorPool([labeller.remote() for i in range(0,N_CPUS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93618fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = os.listdir(\"../precut\")\n",
    "import random\n",
    "random.shuffle(vids)\n",
    "\n",
    "list(actors.map(lambda a, v: a.label_video.remote(v), vids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f120e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e4fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdd2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 13:35:01,752\tERROR import_thread.py:88 -- ImportThread: Error while reading from socket: (10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\n",
      "2021-07-21 13:35:01,756\tERROR worker.py:1125 -- listen_error_messages_raylet: Error while reading from socket: (10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\n",
      "2021-07-21 13:35:01,757\tERROR worker.py:465 -- print_logs: Error while reading from socket: (10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\n"
     ]
    }
   ],
   "source": [
    "for c in clips:\n",
    "    play_clip(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}